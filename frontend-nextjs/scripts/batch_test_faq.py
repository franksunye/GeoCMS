"""
æ‰¹é‡æµ‹è¯•å®¢æˆ·é—®é¢˜æå–èƒ½åŠ› (Batch Test)

åŠŸèƒ½ï¼š
1. è¿æ¥ Supabase æ•°æ®åº“è·å–çœŸå®è½¬å½•æ•°æ®
2. é¢„å¤„ç†æå–å®¢æˆ·è¯­éŸ³
3. æ‰¹é‡è°ƒç”¨æ··å…ƒ Lite API è¿›è¡Œåˆ†æ
4. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
"""

import json
import time
import psycopg2
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

# é…ç½®
HUNYUAN_API_KEY = "sk-t5eMk6ZZSLu3CJlYpMmsPVNQQMcBrjY4N2uxhkfkMP3PgKv0"
HUNYUAN_BASE_URL = "https://api.hunyuan.cloud.tencent.com/v1"
DB_CONNECTION = "postgresql://postgres.ugzelbolcskrwcrgxqjf:2tZNtDeW8nI0NWJR@aws-0-us-west-2.pooler.supabase.com:5432/postgres"

# æ‰©å±•çš„å›ºå®šåˆ†ç±»åˆ—è¡¨ï¼ˆåŸºäºæ–¹æ¡ˆ1ï¼Œå¢åŠ äº†å‡ ä¸ªå¸¸è§ç±»åˆ«ï¼‰
CATEGORIES = [
    "æœåŠ¡èŒƒå›´",    # èƒ½ä¸èƒ½åšXXã€åšä»€ä¹ˆéƒ¨ä½
    "ä»·æ ¼å’¨è¯¢",    # å¤šå°‘é’±ã€æ€ä¹ˆæ”¶è´¹
    "è´¨ä¿æœŸ",      # ä¿ä¿®å¤šä¹…ã€å‡ºé—®é¢˜æ€ä¹ˆåŠ
    "ä¸Šé—¨æ—¶é—´",    # ä»€ä¹ˆæ—¶å€™æ¥ã€èƒ½çº¦ä»€ä¹ˆæ—¶å€™
    "æœåŠ¡äººå‘˜",    # å¸ˆå‚…ä¸“ä¸šå—ã€æ˜¯è‡ªå·±çš„è¿˜æ˜¯å¤–åŒ…
    "æ–½å·¥æµç¨‹",    # æ€ä¹ˆåšã€éœ€è¦å¤šä¹…
    "è”ç³»æ–¹å¼",    # ç•™ç”µè¯ã€åŠ å¾®ä¿¡
    "å…¶ä»–",        # ä¸å±äºä»¥ä¸Šç±»åˆ«
]

# æ–¹æ¡ˆ1ä¼˜åŒ–ç‰ˆ Prompt - å®Œæ•´å¯¹è¯æ ¼å¼ + ä¸¥æ ¼åªæå–é—®é¢˜
EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå®¢æˆ·æœåŠ¡åˆ†æä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹é”€å”®é€šè¯è®°å½•ä¸­ï¼Œæå–å‡ºã€å®¢æˆ·ã€‘æå‡ºçš„é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. åªæå–å®¢æˆ·è¯´çš„é—®é¢˜ï¼ˆå¸¦é—®å·æˆ–ç–‘é—®è¯­æ°”çš„å¥å­ï¼‰
2. ä¸è¦æå–å®¢æˆ·çš„é™ˆè¿°ã€æè¿°ã€ç¡®è®¤æˆ–å›åº”
3. å¯¹æ¯ä¸ªé—®é¢˜è¿›è¡Œåˆ†ç±»ï¼ˆæœåŠ¡èŒƒå›´ã€ä»·æ ¼å’¨è¯¢ã€è´¨ä¿æœŸã€ä¸Šé—¨æ—¶é—´ã€æœåŠ¡äººå‘˜ã€æ–½å·¥æµç¨‹ã€è”ç³»æ–¹å¼ã€å…¶ä»–ï¼‰
4. ä»¥ JSON æ ¼å¼è¾“å‡º

è¾“å‡ºæ ¼å¼ï¼š
```json
{
  "questions": [
    {"question": "å®¢æˆ·åŸè¯", "category": "åˆ†ç±»"}
  ]
}
```

é€šè¯è®°å½•ï¼š
"""

def format_dialog(transcript_json):
    """å°†è½¬å½•æ•°æ®æ ¼å¼åŒ–ä¸ºå®Œæ•´å¯¹è¯æ ¼å¼ï¼ˆæ–¹æ¡ˆ1æ ¸å¿ƒï¼‰"""
    try:
        data = json.loads(transcript_json)
        dialog_lines = []
        for item in data:
            speaker = "é”€å”®" if item.get("SpeakerId") == "1" else "å®¢æˆ·"
            text = item.get("Text", "").strip()
            if text:
                dialog_lines.append(f"[{speaker}] {text}")
        return "\n".join(dialog_lines)
    except:
        return ""

client = OpenAI(api_key=HUNYUAN_API_KEY, base_url=HUNYUAN_BASE_URL)

def get_transcripts(limit=20):
    """ä»æ•°æ®åº“è·å–è½¬å½•æ•°æ®"""
    print(f"ğŸ”Œ è¿æ¥æ•°æ®åº“...")
    try:
        conn = psycopg2.connect(DB_CONNECTION)
        cur = conn.cursor()
        
        # è·å–é•¿åº¦é€‚ä¸­çš„è½¬å½•æ–‡æœ¬
        query = """
            SELECT id, deal_id, content
            FROM sync_transcripts
            WHERE LENGTH(content) > 1000 AND LENGTH(content) < 8000
            ORDER BY RANDOM()
            LIMIT %s;
        """
        cur.execute(query, (limit,))
        rows = cur.fetchall()
        
        print(f"âœ… æˆåŠŸè·å– {len(rows)} æ¡æ•°æ®")
        conn.close()
        return rows
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return []

def preprocess_transcript(transcript_json):
    """é¢„å¤„ç†ï¼šåªæå–å®¢æˆ·è¯´çš„è¯"""
    try:
        data = json.loads(transcript_json)
        customer_texts = []
        for item in data:
            if item.get("SpeakerId") == "2":
                text = item.get("Text", "").strip()
                if text and len(text) > 1:
                    customer_texts.append(f"- {text}")
        return "\n".join(customer_texts)
    except:
        return ""

def analyze_transcript(row):
    """åˆ†æå•æ¡æ•°æ® - æ–¹æ¡ˆ1ï¼šå®Œæ•´å¯¹è¯æ ¼å¼"""
    tid, did, content = row
    
    # æ ¼å¼åŒ–ä¸ºå®Œæ•´å¯¹è¯ï¼ˆæ–¹æ¡ˆ1æ ¸å¿ƒæ”¹åŠ¨ï¼‰
    dialog_text = format_dialog(content)
    if not dialog_text:
        return None
        
    start_time = time.time()
    try:
        response = client.chat.completions.create(
            model="hunyuan-lite",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æˆ·æœåŠ¡åˆ†æåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": EXTRACTION_PROMPT + dialog_text}
            ],
            max_tokens=1000,
            temperature=0.3
        )
        duration = time.time() - start_time
        result = response.choices[0].message.content
        
        # è§£æç»“æœ
        questions = []
        try:
            json_str = result
            if "```json" in result:
                json_str = result.split("```json")[1].split("```")[0]
            elif "```" in result:
                json_str = result.split("```")[1].split("```")[0]
            
            parsed = json.loads(json_str.strip())
            questions = parsed.get("questions", [])
        except:
            pass
            
        return {
            "id": tid,
            "questions": questions,
            "duration": duration,
            "text_length": len(dialog_text)
        }
    except Exception as e:
        print(f"Error analyzing {tid}: {e}")
        return None

def run_batch_test():
    print("ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯• (N=20)...")
    
    # 1. è·å–æ•°æ®
    rows = get_transcripts(20)
    if not rows:
        return
        
    results = []
    
    # 2. å¹¶è¡Œå¤„ç†
    print("âš¡ æ­£åœ¨è°ƒç”¨ AI åˆ†æ...")
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(analyze_transcript, row) for row in rows]
        for future in tqdm(futures, total=len(rows)):
            res = future.result()
            if res:
                results.append(res)
                
    # 3. ç»Ÿè®¡ç»“æœ
    total_questions = sum(len(r["questions"]) for r in results)
    avg_time = sum(r["duration"] for r in results) / len(results) if results else 0
    categories = {}
    
    for r in results:
        for q in r["questions"]:
            # è·³è¿‡éå­—å…¸ç±»å‹ï¼ˆå°æ¨¡å‹æœ‰æ—¶è¿”å›æ ¼å¼ä¸æ­£ç¡®ï¼‰
            if not isinstance(q, dict):
                continue
            # å…¼å®¹ä¸¤ç§æ ¼å¼: q/c æˆ– question/category
            cat = q.get("c") or q.get("category") or "æœªåˆ†ç±»"
            categories[cat] = categories.get(cat, 0) + 1
            
    # 4. è¾“å‡ºæŠ¥å‘Š
    print("\n" + "="*50)
    print("ğŸ“Š æ‰¹é‡æµ‹è¯•æŠ¥å‘Š (ä¼˜åŒ–å)")
    print("="*50)
    print(f"æˆåŠŸåˆ†ææ ·æœ¬æ•°: {len(results)} / {len(rows)}")
    print(f"æå–é—®é¢˜æ€»æ•°: {total_questions}")
    print(f"å¹³å‡æ¯å•è€—æ—¶: {avg_time:.2f} ç§’")
    print(f"å¹³å‡æ¯å•é—®é¢˜æ•°: {total_questions / len(results):.1f}")
    
    print("\nğŸ† é—®é¢˜åˆ†ç±» TOP 10:")
    sorted_cats = sorted(categories.items(), key=lambda x: x[1], reverse=True)
    for cat, count in sorted_cats[:10]:
        print(f"  - {cat}: {count}")
        
    print("\nğŸ’¡ å…¸å‹é—®é¢˜ç¤ºä¾‹:")
    sample_count = 0
    for r in results[:5]:
        for q in r["questions"]:
            if sample_count >= 8: break
            question = q.get("q") or q.get("question") or ""
            category = q.get("c") or q.get("category") or ""
            if len(question) > 5:  # åªæ˜¾ç¤ºæœ‰æ„ä¹‰çš„é—®é¢˜
                print(f"  [{category}] {question}")
                sample_count += 1

if __name__ == "__main__":
    run_batch_test()
